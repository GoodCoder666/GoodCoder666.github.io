<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="前言\r如题，这是我的一个探索型项目。其目的并不是为了达成什么效果，而是在尝试的过程中学习知识。本文包含简要的技术报告，亦有过程中的一些心得体会。评论系统还没弄好，若有任何想法或疑问欢迎直接在项目的 Issue / Discussion 中发出来。\n">
<title>katac4: 基于 AlphaZero 的强化学习探索</title>

<link rel='canonical' href='https://goodcoder666.github.io/p/katac4/'>

<link rel="stylesheet" href="/scss/style.min.b9c8156d464c343bdacaf14a871581fb94cbbdb9dd5cbce4ba017361187cc930.css"><meta property='og:title' content="katac4: 基于 AlphaZero 的强化学习探索">
<meta property='og:description' content="前言\r如题，这是我的一个探索型项目。其目的并不是为了达成什么效果，而是在尝试的过程中学习知识。本文包含简要的技术报告，亦有过程中的一些心得体会。评论系统还没弄好，若有任何想法或疑问欢迎直接在项目的 Issue / Discussion 中发出来。\n">
<meta property='og:url' content='https://goodcoder666.github.io/p/katac4/'>
<meta property='og:site_name' content='GoodCoder&#39;s Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='人工智能' /><meta property='article:tag' content='深度学习' /><meta property='article:tag' content='强化学习' /><meta property='article:published_time' content='2025-06-28T23:49:50&#43;08:00'/><meta property='article:modified_time' content='2025-06-28T23:49:50&#43;08:00'/>
<meta name="twitter:site" content="@stanleys09">
    <meta name="twitter:creator" content="@stanleys09"><meta name="twitter:title" content="katac4: 基于 AlphaZero 的强化学习探索">
<meta name="twitter:description" content="前言\r如题，这是我的一个探索型项目。其目的并不是为了达成什么效果，而是在尝试的过程中学习知识。本文包含简要的技术报告，亦有过程中的一些心得体会。评论系统还没弄好，若有任何想法或疑问欢迎直接在项目的 Issue / Discussion 中发出来。\n">
    <link rel="shortcut icon" href="/favicon.ico" />

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-KJPJLGSR3C"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-KJPJLGSR3C');
        }
      </script>
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu3086030834252304089.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🍥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">GoodCoder&#39;s Blog</a></h1>
            <h2 class="site-description">欢迎来到我的小站！</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/GoodCoder666'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://x.com/stanleys09'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>文章</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于我</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">
                    
                        <li id="i18n-switch">  
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                            <select name="language" title="language" onchange="window.location.href = this.selectedOptions[0].value">
                                
                                    <option value="https://goodcoder666.github.io/en/" >English</option>
                                
                                    <option value="https://goodcoder666.github.io/" selected>中文</option>
                                
                            </select>
                        </li>
                    
                

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#前言">前言</a></li>
    <li><a href="#alphazero-速览">AlphaZero 速览</a></li>
    <li><a href="#训练退火">训练，退火</a></li>
    <li><a href="#算法优化">算法优化</a>
      <ol>
        <li><a href="#梯度累加">梯度累加</a></li>
        <li><a href="#动作空间剪枝">动作空间剪枝</a></li>
        <li><a href="#first-play-urgency-fpu">First Play Urgency (FPU)</a></li>
        <li><a href="#模拟上限随机化">模拟上限随机化</a></li>
        <li><a href="#强制探索--策略剪枝">强制探索 &amp; 策略剪枝</a></li>
        <li><a href="#动态经验回放池">动态经验回放池</a></li>
        <li><a href="#蒙特卡洛图搜索mcgs">蒙特卡洛图搜索（MCGS）</a></li>
      </ol>
    </li>
    <li><a href="#工程优化">工程优化</a>
      <ol>
        <li><a href="#并行自对弈">并行自对弈</a></li>
        <li><a href="#cuda-graphs">CUDA Graphs</a></li>
      </ol>
    </li>
    <li><a href="#测试结果">测试结果</a></li>
    <li><a href="#后续改进方向">后续改进方向</a></li>
    <li><a href="#拓展阅读">拓展阅读</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/katac4/">katac4: 基于 AlphaZero 的强化学习探索</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025/06/28</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 14 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="前言">前言
</h2><p><em>如题，这是我的一个探索型项目。其目的并不是为了达成什么效果，而是在尝试的过程中学习知识。本文包含简要的技术报告，亦有过程中的一些心得体会。评论系统还没弄好，若有任何想法或疑问欢迎直接在项目的 Issue / Discussion 中发出来。</em></p>
<p>最近在研究强化学习（RL），其中特别喜欢 AlphaZero 算法。它是 2016 年击败世界围棋冠军李世石的 AlphaGo 演化而来的通用强化学习算法。一些主流的强化学习介绍书籍一般止步于 PPO、SAC 或 DDPG 等方法（而没有 AlphaZero、MuZero 系列），私以为是一个很大的错误。</p>
<p>从根源上来看，AlphaZero <strong>属于 Model-Based RL 的范畴</strong>，这与以 Q-learning、Actor-Critic 为代表的 Model-Free RL 有本质区别。其在于使用了<strong>环境模型</strong>（游戏模拟器）进行树搜索推演最优策略，而不是通过大量试错来逐步提升预期收益。良好的环境模型对于这类算法的结果至关重要；在棋类游戏中，明确的规则直接给出了一个完美的环境模型。而真实物理世界中的环境十分复杂，难以用简单的规则去描述，这成为了阻碍强化学习技术落地应用的关键瓶颈。Meta 近期开源的 <a class="link" href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/"  target="_blank" rel="noopener"
    >V-JEPA 2</a> 就是构建世界模型的重要一步，非常建议大家去进一步了解。</p>
<p>受计算资源的限制，我选择了 <a class="link" href="https://www.saiblo.net/game/3"  target="_blank" rel="noopener"
    >Saiblo 四子棋</a> 作为游戏环境 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，状态空间相对较小，但也有一定的复杂性，没有已知必胜策略。另外平台上也有非常优秀的传统方法用于比较，可以获得一个相对客观的算法棋力评测。在此非常感谢清华大学提供的平台，在满足校内同学实验需求的同时能给校外的爱好者带来很大便利。</p>
<p>由于我的 AI 使用了 <a class="link" href="https://github.com/lightvector/KataGo"  target="_blank" rel="noopener"
    >KataGo</a> 中的一些算法改进，故命名为 katac4，代码在 <a class="link" href="https://github.com/GoodCoder666/katac4"  target="_blank" rel="noopener"
    >GitHub</a> 上完全开源。</p>
<p>截至文章写作时间（2025.06.27），katac4 在游戏天梯上以遥遥领先的分数稳居榜一：</p>
<p><img src="/p/katac4/rank.png"
	width="1756"
	height="948"
	srcset="/p/katac4/rank_hu3571190474406266030.png 480w, /p/katac4/rank_hu8101580597091962575.png 1024w"
	loading="lazy"
	
		alt="6月27日的四子棋游戏天梯"
	
	
		class="gallery-image" 
		data-flex-grow="185"
		data-flex-basis="444px"
	
></p>
<h2 id="alphazero-速览">AlphaZero 速览
</h2><p>网上通俗易懂地介绍 AlphaZero 的文章不在少数，因此这里仅作较为抽象的简要概述（思路参考 KataGo 论文）。</p>
<p>算法通过神经网络引导的蒙特卡洛树搜索（MCTS）进行自我对弈以生成训练数据。搜索过程即反复扩展游戏树：每次从根节点出发沿一条链走到叶子节点，在节点 $n$ 处选择使以下式子最大的子节点 $c$ 进行访问：</p>
$$
\mathrm{PUCT}(c)=V(c)+c_\mathrm{PUCT}P(c) \frac{\sqrt{\sum_{c'}N(c')}}{1+N(c)}
$$<p>其中 $V(c)$ 是 $c$ 子树中所有节点的平均预测效用值，$P(c)$ 是神经网络给出的 $c$ 对应动作先验概率，$N(c)$ 为之前通过节点 $c$ 进行模拟的次数，常数 $c_\mathrm{PUCT}=1.1$；$c'$ 表示 $n$ 的所有子节点。</p>
<p>不同于 $\epsilon$-greedy 算法，AlphaZero 通过对根节点处的先验概率施加 Dirichlet 噪声以增强探索：</p>
$$
P(c)=0.75P_{\text{raw}}(c)+0.25\eta
$$<p>其中 $\eta$ 采样自参数 $\alpha=0.8$ 的 Dirichlet 分布，$P_{\text{raw}}$ 表示神经网络给出的原始策略分布。</p>
<p>设搜索树的根有 $k$ 个子节点 $u_1,\dots,u_k$，它们对应的动作分别为 $a_1,\dots,a_k$。则 MCTS 的 <em>visit distribution</em> 正比于 $N(u_i)$：</p>
$$
\pi(a_k|s_t)=\frac{N(u_k)}{\sum_{i=1}^k N(u_i)}
$$<p>这是神经网络的训练目标，同时用于选择自对弈时的下一个动作（以动态变化的温度 $T$ 采样）。<strong>自对弈的起点为初始局面</strong>。在推理（实战）阶段，一般直接选择该分布中概率最大的动作，也就是访问次数最多的。</p>
<p>神经网络接受当前的局面状态 $s_t$ 作为输入，预测下一个动作的概率分布 $\pi$ 和局面胜率 $z$。其损失函数为：</p>
$$
L=-c_g \sum_r z(r) \log(\hat z(r))-\sum_m \pi(m) \log(\hat \pi(m))+c_{L2} ||\boldsymbol\theta||^2
$$<p>其中 $r \in \{\text{win}, \text{loss}, \text{draw}\}$ 表示当前玩家视角下的游戏结果，常数 $c_g=1.5,c_{L2}=3\times 10^{-5}$。整个损失函数即为以下三部分的加权和：</p>
<ol>
<li>胜率预测与实际结果的 Cross Entropy <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>；</li>
<li>策略分布预测与 MCTS visit distribution 的 Soft Cross Entropy；</li>
<li>对神经网络参数 $\boldsymbol\theta$ 的 L2 正则化。</li>
</ol>
<p>策略更新采用 <strong>Off-policy</strong> 的方式，维护经验回放池（replay buffer），每轮自对弈完成后将新的经验数据加入回放池，再从中采样一个大小为 $B$ 的小批量进行梯度下降更新。使用 SGD with Momentum 优化器，学习率随训练阶段调整。</p>
<p>整体上，AlphaZero 与<strong>策略迭代算法</strong>的核心思想是一致的，只是用 MCTS 同时完成了<strong>策略评估</strong>和<strong>策略提升</strong>的任务。</p>
<h2 id="训练退火">训练，退火
</h2><p><strong>退火</strong>的思想对于成功训练一个 AlphaZero Agent 至关重要。这里涉及到非常多的细节，建议大家参考代码，我在此列出几个关键要点：</p>
<ol>
<li>从 visit distribution 中采样动作的温度 $T=0.8^{1+step/boardsize}$。<a class="link" href="https://github.com/junxiaosong/AlphaZero_Gomoku"  target="_blank" rel="noopener"
    >AlphaZero_Gomoku</a> 中使用常数 $T=1$，实践证明这是完全错误的。在一盘棋后期很容易”一招不慎，满盘皆输“，若直接根据 visit distribution 采样动作会产生大量错误 game outcome 样本，导致整个训练崩溃。</li>
<li>神经网络给出的策略，在根节点应用温度 $T=\max(1.03,1.35\times0.66^{step/boardsize})$。这可以确保一定的探索性，增强训练稳定性。</li>
<li>学习率退火。训练的前期（$5\%$ 迭代次数）使用 $1/3$ 的正常学习率（防止前期训练大幅波动），$72\%$ 以后使用 $1/10$ 的正常学习率（最大化提升棋力）。</li>
</ol>
<p>很多超参数不一定最优，都是随手设置的，但是它们的组合确实有效，希望能给大家提供一些参考价值。</p>
<h2 id="算法优化">算法优化
</h2><blockquote>
<p>Recomputing the AlphaGo Zero weights will <a class="link" href="http://web.archive.org/web/20190205013627/http://computer-go.org/pipermail/computer-go/2017-October/010307.html"  target="_blank" rel="noopener"
    >take about 1700 years on commodity hardware</a>.</p>
</blockquote>
<p>这句话来自 Leela Zero 项目的 README 文档。AlphaZero 本身的探索过程十分低效，但是利用精心设计的算法优化可以显著缩短其训练周期。<a class="link" href="https://arxiv.org/abs/1902.10565"  target="_blank" rel="noopener"
    >KataGo 论文</a>中的方法共同作用，可将训练效率提升 ~50x。下面介绍我使用的一些算法优化，其中部分来自 KataGo。</p>
<h3 id="梯度累加">梯度累加
</h3><p>我们的游戏环境——Saiblo 四子棋的棋盘大小不固定，长宽均在 $[9,12]$ 区间内取值。因此共有 $16$ 种不同大小的棋盘。</p>
<p>为了适应不同的棋盘大小，做以下两点改进：</p>
<ol>
<li>神经网络采用全卷积（FCN）架构<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，策略头输出与输入状态大小保持一致。</li>
<li>从 replay buffer 中采样时，由直接采样 $256$ 个样本改为每种棋盘大小各采样 $16$ 个样本。<strong>对不同棋盘大小的样本各进行一次 forward pass，计算梯度并累加</strong>。然后正常 backward + step 即可。</li>
</ol>
<p>KataGo 中采用的是 board masking 的方式适应不同输入，而这涉及到繁琐的细节实现。直接采用最简单的梯度累加来完成这一点，虽效率稍低但方便了许多。</p>
<h3 id="动作空间剪枝">动作空间剪枝
</h3><p>这个方法的动机非常简单：人工筛选掉可以被证明严格错误的着法，将其视为不合法动作，排除出 MCTS 计算。</p>
<p>实现只需非常小的代码更改，总结为两条规律：</p>
<ol>
<li>我走一步棋直接赢，则必走；</li>
<li>对手走一步棋直接赢，则我必堵。</li>
</ol>
<p>这种优化非常简单，却能显著加快训练前期探索，让模型迅速理解游戏规则。同时，这也能避免一些很糟糕的盲点，尤其是对手在棋盘边缘成三有可能会被 CNN 忽视。</p>
<h3 id="first-play-urgency-fpu">First Play Urgency (FPU)
</h3><blockquote>
<p>这项优化源自 Leela Zero。</p>
</blockquote>
<p>细心的读者肯定会问：PUCT 计算时，若子节点没有访问即 $N(c)=0$，$V(c)$ 怎么定义？</p>
<p>AlphaGo Zero 给出了标准答案：$V(c)=0$。但 Leela Zero 社区讨论发现，这并不是最优选择。他们选择：</p>
$$
V(c)=V(n)-c_\text{FPU}\sqrt{P_\text{explored}}
$$<p>其中 $n$ 为父亲节点，常数 $c_\text{FPU}=0.2$，$P_\text{explored}$ 为访问过至少一次的子节点的策略先验概率总和：</p>
$$
P_\text{explored}=\sum_{c'|N(c')>0} P(c')
$$<p>读者可以自行思考这种做法的合理性。<a class="link" href="https://github.com/glinscott/leela-chess/issues/160#issuecomment-374912061"  target="_blank" rel="noopener"
    >Lc0 的一个讨论</a>指出，对于实力较强的神经网络，此法足以带来 ~200 Elo 的提升。</p>
<h3 id="模拟上限随机化">模拟上限随机化
</h3><blockquote>
<p>对应 KataGo 论文中的 <em>Playout Cap Randomization</em>。</p>
</blockquote>
<p><strong>AlphaZero 中的两个输出头训练所需 playouts 并不相同</strong>。一些非正式研究表明，训练 policy head 最高效的模拟次数设置与 AlphaZero 的 $N=800$ 十分接近，但在 AlphaGo 的第一个版本中只用 $N=1$ 也能训练出较好的 value head。为了缓解二者的紧张关系，我们取一个较小的模拟次数 $n < N$，并以概率 $p$ 执行 fast search（$n$ 次模拟），其余情况正常 full search（$N$ 次模拟）。<strong>fast search 产生的样本不用于策略训练<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>。</strong></p>
<p>我的超参数选择是 $(N,n)=(800,160)$，以及 $p=0.25$。个人感觉对于四子棋这种游戏，模拟次数再小一点应该也没事。</p>
<h3 id="强制探索--策略剪枝">强制探索 &amp; 策略剪枝
</h3><blockquote>
<p>对应 KataGo 论文中的 <em>Forced Playouts and Policy Target Pruning</em>。</p>
</blockquote>
<p>前面提到，为了增强探索，AlphaZero 在根节点处引入 Dirichlet noise。然而这种方式并不能保证好的着法一定能被发现。</p>
<p>设想下面这个场景：</p>
<ol>
<li>游戏状态 $s_t$。当前的神经网络十分青睐着法 $a$，而实际最优着法为 $a^*$。在输出的策略分布中，$a$ 获得了高于 $80\%$ 的概率，$a^*$ 不到 $5\%$。</li>
<li>引入 Dirichlet noise。$a$ 的概率仍然很大，$a^*$ 被“选中”来到 $10\%$。假设此时 $P(a)$ 正好为 $80\%$。</li>
<li>MCTS 依次访问 $a$ 和 $a^*$。此时，价值网络仍不看好 $a^*$ 走到的状态 $s^*$，还是倾向于 $a$ 走到的状态 $s$，给出 $V(s^*)=-0.8$ 和 $V(s)=-0.1$。</li>
<li>现在的总模拟次数 $N=2$。根据 PUCT 公式可知 $\text{PUCT}(a)=0.34,\text{PUCT}(a^*)=-0.745$。这时二者 PUCT 值已经差距巨大，若 MCTS 不断选择 $a$ 进行访问并保持 $V(s)$ 不变，要到将近 $200$ 次模拟的时候才会使 $\text{PUCT}(a^*)\ge\text{PUCT}(a)$，此时 $1/4$ 的模拟次数已经被浪费了；并且如果 $a^*$ 再一次被给予差评，又要等上很长一段时间才能获得下一次访问。</li>
</ol>
<p>由此可见，即使 Dirichlet noise 运气很好落在了正确的着法上，策略和价值网络共同的盲点仍然可能未被正确处理。故引入强制探索（Forced Playouts）机制，确保根节点的每个孩子都获得至少 $\lceil n_\text{forced} \rceil$ 次访问：</p>
$$
n_\text{forced}(c)=\left(kP(c)\sum_{c'}N(c')\right)^{1/2}
$$<p><br>
沿用 KataGo 的设置，常数 $k=2$。</p>
<p>与此同时，为了消去 Dirichlet noise 和 Forced Playouts 共同带来的大量噪声，我们在 MCTS 完成后对 visit distribution 进行策略剪枝：</p>
<ol>
<li>找到访问次数 $N(c)$ 最多的孩子 $c^*$。</li>
<li>对于其他子节点 $c$，减去尽可能多的访问次数，确保 $\text{PUCT}(a^*) > \text{PUCT}(a)$ 且减掉的次数不超过 $n_\text{forced}(c)$。</li>
</ol>
<p>我这里选择直接使用不等式解出剪枝后的访问次数：</p>
$$
N'(c)=\mathrm{clip}\left(\left\lceil\frac{P\sqrt N}{V+\text{PUCT}(c^*)}-1\right\rceil,N-n_\text{forced},N\right)
$$<p>为了方便阅读，以上公式中直接用 $N$ 指代 $N(c)$，$P$ 指代 $P(c)$，etc.</p>
<p>以下使用/不使用策略剪枝的效果对比来自 KataGo 论文。黑：$p \approx 2\times 10^{-4}$；绿： $p \approx 1$。</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center"><img src="/p/katac4/logpolicy1.png"
	width="684"
	height="686"
	srcset="/p/katac4/logpolicy1_hu14807721752196513033.png 480w, /p/katac4/logpolicy1_hu17779249405163364952.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="99"
		data-flex-basis="239px"
	
></th>
          <th style="text-align: center"><img src="/p/katac4/logpolicy2.png"
	width="684"
	height="686"
	srcset="/p/katac4/logpolicy2_hu16634519969668677271.png 480w, /p/katac4/logpolicy2_hu3778008593870311584.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="99"
		data-flex-basis="239px"
	
></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">使用策略剪枝</td>
          <td style="text-align: center">不使用策略剪枝</td>
      </tr>
  </tbody>
</table></div>
<h3 id="动态经验回放池">动态经验回放池
</h3><blockquote>
<p>来自 KataGo 论文中 <em>Appendix C: Training Details</em>。</p>
</blockquote>
<p>Off-policy 的强化学习算法中，经验回放池的大小（容纳样本数）大多是固定的，常设置在 $[2^{14},2^{20}]$ 区间内。而我们采取一种亚线性的增长策略：</p>
$$
N_{\text{window}} = c \left( 1 + \beta \frac{ ( N_{\text{total}} / c ) ^ \alpha - 1} { \alpha } \right)
$$<p>其中 $N_{\text{total}} $ 为当前训练过程生成的样本总数，$c=250000,\alpha=0.75,\beta=0.4$。这实际上就是对 $f(n)=n^\alpha$ 应用线性变换，使得 $f(c)=c$ 且 $f'(c)=\beta$。这样就能快速舍弃前期产生的低质量着法，并在后期增加训练样本多样性，有效抑制过拟合。</p>
<h3 id="蒙特卡洛图搜索mcgs">蒙特卡洛图搜索（MCGS）
</h3><p>见 <a class="link" href="https://github.com/lightvector/KataGo/blob/master/docs/GraphSearch.md"  target="_blank" rel="noopener"
    >KataGo/docs/GraphSearch.md</a>。非常感谢 David Wu 提供的通俗易懂的讲解；我在做这个项目过程中还发现了其中一处 typo 并 PR 修复了 &#x1f606;</p>
<p>整体思路：</p>
<ol>
<li>
<p>对局面实现 Zobrist hash，扩展叶子时优先从哈希表中查找相同状态节点，搜索树变为 DAG；</p>
</li>
<li>
<p><strong>依据 action edge（而不是 node）的访问次数来计算 PUCT</strong>；</p>
</li>
<li>
<p>反向传播过程采用<strong>增量更新</strong>：</p>
$$
   V(n) \larr \frac{1}{N(n)} \left(U(n)+\sum_c N(c)V(c)\right)
   $$<p>注意 $U(n)$ 表示价值网络给出的 utility estimate，是必不可少的。</p>
</li>
</ol>
<p>实际操作中，MCGS 在内存回收方面会比较麻烦，好在 AlphaZero 产生的搜索树不是很大，所以我直接做一遍 DFS 清理。目前训练阶段还是使用 MCTS，推理使用 MCGS。</p>
<h2 id="工程优化">工程优化
</h2><p>由于项目代码是 100% Python，要达到资源的完全利用几乎不可能。不过还是有一些技巧可以提升训练效率的。</p>
<h3 id="并行自对弈">并行自对弈
</h3><p>这个 idea 很好理解，也可以应用到大部分主流强化学习算法上。使用多个进程并行在不同的 GPU 上收集训练数据，并将结果发送给主进程进行模型更新。</p>
<p>目前，我总共使用 $20$ 个不同的进程自对弈，均匀分配到 $4$ 张卡上运行。注意要使用 <code>torch.multiprocessing</code> 代替 Python 内置的 <code>multiprocessing</code> 模块。</p>
<h3 id="cuda-graphs">CUDA Graphs
</h3><p>这也是一个比较通用的优化技巧，在很多机器学习应用中都可以使用。其原理是<strong>将整个计算图发送到 GPU 上执行，显著减少前/后端交互和 kernel 启动开销 <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></strong>。对于较小的网络和 batch_size，CUDA Graph 可以带来意料之外的速度提升。</p>
<p>项目根目录下提供了（简陋的）benchmark 脚本，在单张 RTX 4090 上测试结果如下（<code>batch_size=1</code> 模拟 self-play 环境）：</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th>类别</th>
          <th style="text-align: left">推理方法</th>
          <th style="text-align: center">计算速度（FPS）</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Base</td>
          <td style="text-align: left"><code>torch.no_grad</code></td>
          <td style="text-align: center">$249.1$</td>
      </tr>
      <tr>
          <td>Base</td>
          <td style="text-align: left"><code>torch.inference_mode</code></td>
          <td style="text-align: center">$265.9$</td>
      </tr>
      <tr>
          <td>TorchScript</td>
          <td style="text-align: left"><code>torch.jit.script</code></td>
          <td style="text-align: center">$271.5$</td>
      </tr>
      <tr>
          <td>TorchScript</td>
          <td style="text-align: left"><code>torch.jit.trace</code></td>
          <td style="text-align: center">$501.4$</td>
      </tr>
      <tr>
          <td>CUDA Graphs</td>
          <td style="text-align: left"><code>torch.cuda.graph</code></td>
          <td style="text-align: center">$3184.5$</td>
      </tr>
  </tbody>
</table></div>
<p>另外 <code>torch.compile(mode='reduce-overhead')</code> 也可以实现基于 CUDA Graph 的推理。不过这个 API 不支持 Windows，且编译耗时，没有 <code>torch.cuda.graph</code> 用的舒服。</p>
<h2 id="测试结果">测试结果
</h2><p>训练持续约 $14$ 天，共 $30000 \times 16$ 次迭代。首先是训练过程的 <strong>Loss</strong>、<strong>Entropy</strong>（策略熵）和 <strong>episode_len</strong>（自对弈棋局长度）曲线：</p>
<p><img src="/p/katac4/loss.png"
	width="1977"
	height="571"
	srcset="/p/katac4/loss_hu8748415948911956082.png 480w, /p/katac4/loss_hu12831189363981674269.png 1024w"
	loading="lazy"
	
		alt="Total loss / Policy loss / Value loss (Smoothing = 0.9)"
	
	
		class="gallery-image" 
		data-flex-grow="346"
		data-flex-basis="830px"
	
></p>
<p><img src="/p/katac4/entropy_episode_len.png"
	width="1317"
	height="563"
	srcset="/p/katac4/entropy_episode_len_hu6489537421821007243.png 480w, /p/katac4/entropy_episode_len_hu3838643575367717094.png 1024w"
	loading="lazy"
	
		alt="Entropy / Episode_len (Smoothing = 0.99)"
	
	
		class="gallery-image" 
		data-flex-grow="233"
		data-flex-basis="561px"
	
></p>
<p>loss 和 entropy 前期快速下降，后期趋于稳定（不会向 $0$ 收敛是由于数据分布在不断改变）；episode_len 前期提升，后期略有下降（可能是因为找到了更快的杀棋走法）。总体来说符合直觉，没有出现什么不同于预期的表现。</p>
<p>下面是 <strong>ELO</strong><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 评分曲线，体现真实棋力：</p>
<p><img src="/p/katac4/elo.png"
	width="640"
	height="480"
	srcset="/p/katac4/elo_hu448779931538444639.png 480w, /p/katac4/elo_hu6305396987888430977.png 1024w"
	loading="lazy"
	
	
		class="gallery-image" 
		data-flex-grow="133"
		data-flex-basis="320px"
	
></p>
<p>整体来看训练过程十分稳定（近似亚线性增长），最终训练基本饱和。</p>
<p>解决相同任务的<a class="link" href="https://github.com/Lionjump0723/connect4/"  target="_blank" rel="noopener"
    >最优传统方法</a>在相同算力下对 katac4 的最佳检查点只有约 $26\%$ 的胜率，对应 -182 ELO 分差。画在图上可以看出，我们只用 $1/4$ 的迭代次数（$3$ 天训练）就达到了与之相当的水平。</p>
<p><img src="/p/katac4/elo-compare.png"
	width="640"
	height="480"
	srcset="/p/katac4/elo-compare_hu16535128062284470455.png 480w, /p/katac4/elo-compare_hu620230431038270330.png 1024w"
	loading="lazy"
	
		alt="对比 Previous SOTA"
	
	
		class="gallery-image" 
		data-flex-grow="133"
		data-flex-basis="320px"
	
></p>
<p>实际上这个估计并不准确——Epoch 6500 的检查点也能对其取得超过 $50\%$ 的胜率。<em>上图仅供参考。</em></p>
<p>我非常感激这位前辈能在最近开源出他的方法；在项目开发前期，我一直很好奇他为什么能在榜上遥遥领先。前两天看了他的项目报告，真没想到传统方法用 PUCT 也比 UCT 效果更好。说实话，不让我用神经网络，估计连总榜第一页都进不去。</p>
<p>附上对战 Saiblo 平台样例 AI 的测试结果：</p>
<p><img src="/p/katac4/saiblo-test.png"
	width="1104"
	height="298"
	srcset="/p/katac4/saiblo-test_hu16771255767343640165.png 480w, /p/katac4/saiblo-test_hu12574235412547701487.png 1024w"
	loading="lazy"
	
		alt="Epoch 15500 检查点对样例 AI 大获全胜"
	
	
		class="gallery-image" 
		data-flex-grow="370"
		data-flex-basis="889px"
	
></p>
<p>这个结果有时跑不出来，会 TLE。最强的 checkpoint 除了 TLE 之外从来就没输过（测了 $6\times100$ 场）。平台上只有 CPU，因此推理方案用的是 TorchScript。然而 <code>import torch</code> 和 <code>torch.jit.load</code> 都需要时间开销，评测机负载比较大的时候可能还没加载完模型就 TLE 了，这个我真的无能为力。（叹气）</p>
<p>给平台几个建议：</p>
<ol>
<li>支持 LibTorch，这样我可以用 C++ 写推理，排除掉 import 时间；</li>
<li>提供一些轻量化的推理框架（比如 ONNX Runtime），也可以缓解加载缓慢问题。</li>
<li>再退一步，可以适当放宽第一步的时间限制……</li>
</ol>
<p>另外我还搞了一个娱乐性的版本 fastc4，直接选择神经网络输出概率最大的着法。这个版本其实也不赖：</p>
<ul>
<li>对样例 AI 胜率 $97\%$；</li>
<li>对 <a class="link" href="https://github.com/jiegec/FourChess"  target="_blank" rel="noopener"
    >jiegec/FourChess</a> 胜率 $65\%$；</li>
<li>对 frvdec（前榜二）胜率 $48\%$；</li>
<li>对 <a class="link" href="https://github.com/Lionjump0723/connect4/"  target="_blank" rel="noopener"
    >Lionjump0723/connect4</a>（前榜一）胜率 $45\%$。</li>
</ul>
<p>手动观察了一些对局，感觉神经网络更在乎长期收益——它们会为未来很多步规划，而不在乎局部得失。传统方法则天然侧重于短期算杀，而在无明显杀棋时就显得弱势很大。我的 AI 输给其他 AI 经常是在前 $25$ 步因为杀棋盲点被秒掉，但到 $50$ 步以后，局面几乎完全落入它的的控制中，传统算法由于前期规划失误后期根本无能为力。这有点类似于国际象棋引擎中 Stockfish 和 Leela Chess Zero 的区别——前者精打细算，后者更依赖直觉。</p>
<p>希望这些 insight 能为后续传统方法 AI 的开发提供一些帮助，也欢迎大家在平台上与 katac4 对局、测试：</p>
<ul>
<li><strong>katac4（Epoch 29000）</strong>：<code>96c96ac2389547958141d932d9279efc</code></li>
<li>katac4（Epoch 30000）：<code>2c9bd80e1e0e480a8f32214448880a62</code></li>
<li>katac4（Epoch 6500）：<code>d4e85acaf1ab4025b3c6a7ebec4fd0f0</code></li>
<li>fastc4（Epoch 29000）：<code>941dafdce03640bfb7ceb3aa32613252</code></li>
</ul>
<h2 id="后续改进方向">后续改进方向
</h2><p>现在主要问题有二：</p>
<ol>
<li>硬件利用效率低；</li>
<li>模型前期盲点多。</li>
</ol>
<p>后续会考虑将不同自对弈对局中的叶子状态合并到一个大 batch 中，应该能基本解决问题一；</p>
<p>对于问题二，除了扩大模型规模（预计下一个模型为 <code>b5c128nbt</code>）之外，还有一些有待实现的算法优化：</p>
<ol>
<li>添加辅助策略头（对手下一步策略分布、棋局最后落子点、soft policy）帮助训练；</li>
<li>优化棋盘状态表示（前面更多手的落子位置、棋子成二成三的位置等）；</li>
<li>Playout Cap Randomization 的 fast game policy sample 要弃掉；</li>
<li>正比于 $D_\mathrm{KL}(\hat{\boldsymbol\pi}||\boldsymbol\pi)$ 进行 importance sampling，重点训练预测错误的位置；</li>
<li>Batch Norm 替换成 Batch Renorm，防止模型训练和推理的行为不一致。</li>
</ol>
<p>这个暑假如果有空会来实现一些。各位若有兴趣也可以 PR 实现改进。随时欢迎！</p>
<h2 id="拓展阅读">拓展阅读
</h2><ul>
<li><a class="link" href="https://www.nature.com/articles/nature24270"  target="_blank" rel="noopener"
    >Mastering the game of Go without human knowledge</a></li>
<li><a class="link" href="https://arxiv.org/abs/1712.01815"  target="_blank" rel="noopener"
    >Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a></li>
<li><a class="link" href="https://arxiv.org/pdf/1902.10565"  target="_blank" rel="noopener"
    >Accelerating Self-Play Learning in Go</a></li>
<li><a class="link" href="https://github.com/lightvector/KataGo/blob/master/docs/KataGoMethods.md"  target="_blank" rel="noopener"
    >Other Methods Implemented in KataGo</a></li>
<li><a class="link" href="https://hackmd.io/@yrHb-fKBRoyrKDEKdPSDWg/BJgfay0Yc"  target="_blank" rel="noopener"
    >圍棋引擎 Sayuri 開發日誌</a></li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>游戏规则见 <a class="link" href="https://docs.saiblo.net/games/connect4/connect4.html"  target="_blank" rel="noopener"
    >文档</a>。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>原版 AlphaZero 中直接预测局面价值 $V(s_t) \in [-1,1]$ 并使用 MSE Loss，这里沿用了 KataGo 和 Leela Zero 的改进做法。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>神经网络设计也参考了 KataGo，使用了全局池化（Global Pooling）和 <a class="link" href="https://github.com/lightvector/KataGo/blob/master/docs/KataGoMethods.md#nested-bottleneck-residual-nets"  target="_blank" rel="noopener"
    >Nested Bottleneck Residual Nets</a>。现在的版本命名为 <code>b3c128nbt</code>，表示 $3$ 个 $128$ 通道的 nested bottleneck 残差块。出于篇幅考虑，这里不具体介绍模型方面的设计优化，感兴趣的读者可自行翻阅<a class="link" href="https://github.com/GoodCoder666/katac4/blob/main/model.py"  target="_blank" rel="noopener"
    >代码</a>。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>我的代码实现没有做到这一点，看起来影响并不大。不过把它们去掉应该是更好的选择，后续计划测试。&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a class="link" href="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/"  target="_blank" rel="noopener"
    >Accelerating PyTorch with CUDA Graphs – PyTorch</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a class="link" href="https://en.wikipedia.org/wiki/Elo_rating_system"  target="_blank" rel="noopener"
    >Elo rating system - Wikipedia</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
        
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
        
            <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 GoodCoder&#39;s Blog
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
